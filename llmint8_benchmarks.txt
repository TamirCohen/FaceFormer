--------------------- BASE CUDA ------------------------
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------------------------------------------  
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  Source Location                                                              
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------------------------------------------  
                     aten::_native_multi_head_attention         0.52%      42.714ms         2.21%     183.175ms     530.942us       0.000us         0.00%      44.806ms     129.872us           0 b           0 b      14.61 Mb    -685.86 Mb           345  ...in method _native_multi_head_attention of type object at 0x782330ca4880>  
                                                                                                                                                                                                                                                             torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                                                                                                                             nn.Module: MultiheadAttention_0                                              
                                                                                                                                                                                                                                                             torch/nn/modules/transformer.py(723): _sa_block                              
                                                                                                                                                                                                                                                             faceformer.py(71): quantize_decoder_forward                                  
                                                                                                                                                                                                                                                                                                                                          
                     aten::scaled_dot_product_attention         0.04%       3.096ms         1.32%     109.206ms     316.539us       0.000us         0.00%      41.458ms     120.168us           0 b           0 b     122.51 Mb    -299.66 Mb           345  <built-in function scaled_dot_product_attention>                             
                                                                                                                                                                                                                                                             torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                                                                                                                             torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                                                                                                                             nn.Module: MultiheadAttention_1                                              
                                                                                                                                                                                                                                                             torch/nn/modules/transformer.py(733): _mha_block                             
                                                                                                                                                                                                                                                                                                                                          
               aten::_scaled_dot_product_attention_math         0.09%       7.305ms         1.28%     106.110ms     307.565us       0.000us         0.00%      41.458ms     120.168us           0 b      -1.16 Kb     422.17 Mb    -453.54 Mb           345  <built-in function scaled_dot_product_attention>                             
                                                                                                                                                                                                                                                             torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                                                                                                                             torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                                                                                                                             nn.Module: MultiheadAttention_1                                              
                                                                                                                                                                                                                                                             torch/nn/modules/transformer.py(733): _mha_block                             
                                                                                                                                                                                                                                                                                                                                          
                                           aten::matmul         0.12%      10.006ms         0.68%      56.299ms      81.593us       0.000us         0.00%      27.350ms      39.638us           0 b           0 b     421.18 Mb           0 b           690  <built-in function scaled_dot_product_attention>                             
                                                                                                                                                                                                                                                             torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                                                                                                                             torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                                                                                                                             nn.Module: MultiheadAttention_1                                              
                                                                                                                                                                                                                                                             torch/nn/modules/transformer.py(733): _mha_block                             
                                                                                                                                                                                                                                                                                                                                          
                                              aten::bmm         0.32%      26.551ms         0.41%      33.815ms      49.007us      25.291ms        10.12%      27.350ms      39.638us           0 b           0 b     421.18 Mb     421.18 Mb           690  <built-in function scaled_dot_product_attention>                             
                                                                                                                                                                                                                                                             torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                                                                                                                             torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                                                                                                                             nn.Module: MultiheadAttention_1                                              
                                                                                                                                                                                                                                                             torch/nn/modules/transformer.py(733): _mha_block                             
                                                                                                                                                                                                                                                                                                                                          
                                         aten::_to_copy         0.04%       3.223ms         1.11%      91.652ms     265.658us       0.000us         0.00%      22.707ms      65.817us           0 b           0 b     263.91 Mb           0 b           345  <built-in method to of Tensor object at 0x7821f09de110>                      
                                                                                                                                                                                                                                                             faceformer.py(209): predict                                                  
                                                                                                                                                                                                                                                             demo.py(179): test_model                                                     
                                                                                                                                                                                                                                                             torch/utils/_contextlib.py(115): decorate_context                            
                                                                                                                                                                                                                                                             demo.py(368): main                                                           
                                                                                                                                                                                                                                                                                                                                          
                                            aten::copy_         0.05%       3.930ms         1.03%      84.959ms     246.258us      22.381ms         8.96%      22.707ms      65.817us           0 b           0 b           0 b           0 b           345  <built-in method to of Tensor object at 0x7821f09de110>                      
                                                                                                                                                                                                                                                             faceformer.py(209): predict                                                  
                                                                                                                                                                                                                                                             demo.py(179): test_model                                                     
                                                                                                                                                                                                                                                             torch/utils/_contextlib.py(115): decorate_context                            
                                                                                                                                                                                                                                                             demo.py(368): main                                                           
                                                                                                                                                                                                                                                                                                                                          
                                               aten::to         0.05%       4.081ms         1.14%      94.412ms     273.658us       0.000us         0.00%      22.458ms      65.096us           0 b           0 b     263.91 Mb       2.61 Mb           345  <built-in method to of Tensor object at 0x7821f09de110>                      
                                                                                                                                                                                                                                                             faceformer.py(209): predict                                                  
                                                                                                                                                                                                                                                             demo.py(179): test_model                                                     
                                                                                                                                                                                                                                                             torch/utils/_contextlib.py(115): decorate_context                            
                                                                                                                                                                                                                                                             demo.py(368): main                                                           
                                                                                                                                                                                                                                                                                                                                          
                       Memcpy HtoD (Pageable -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us      22.381ms         8.96%      22.381ms      64.872us           0 b           0 b           0 b           0 b           345  <built-in method to of Tensor object at 0x7821f09de110>                      
                                                                                                                                                                                                                                                             faceformer.py(209): predict                                                  
                                                                                                                                                                                                                                                             demo.py(179): test_model                                                     
                                                                                                                                                                                                                                                             torch/utils/_contextlib.py(115): decorate_context                            
                                                                                                                                                                                                                                                             demo.py(368): main                                                           
                                                                                                                                                                                                                                                                                                                                          
                                              aten::bmm         0.29%      24.018ms         0.37%      30.818ms      44.664us      16.968ms         6.79%      18.972ms      27.496us           0 b           0 b     263.91 Mb     263.91 Mb           690  ...in method _native_multi_head_attention of type object at 0x782330ca4880>  
                                                                                                                                                                                                                                                             torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                                                                                                                             nn.Module: MultiheadAttention_0                                              
                                                                                                                                                                                                                                                             torch/nn/modules/transformer.py(723): _sa_block                              
                                                                                                                                                                                                                                                             faceformer.py(71): quantize_decoder_forward                                  
                                                                                                                                                                                                                                                                                                                                          
                                  volta_sgemm_32x128_nn         0.00%       0.000us         0.00%       0.000us       0.000us      17.833ms         7.14%      17.833ms      54.204us           0 b           0 b           0 b           0 b           329  <built-in function scaled_dot_product_attention>                             
                                                                                                                                                                                                                                                             torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                                                                                                                             torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                                                                                                                             nn.Module: MultiheadAttention_1                                              
                                                                                                                                                                                                                                                             torch/nn/modules/transformer.py(733): _mha_block                             
                                                                                                                                                                                                                                                                                                                                          
                                            aten::addmm         0.18%      14.840ms         0.31%      25.275ms      73.261us      12.441ms         4.98%      13.093ms      37.951us           0 b           0 b     172.50 Kb    -344.83 Mb           345  <built-in function linear>                                                   
                                                                                                                                                                                                                                                             torch/nn/modules/linear.py(113): forward                                     
                                                                                                                                                                                                                                                             nn.Module: Linear_78                                                         
                                                                                                                                                                                                                                                             faceformer.py(209): predict                                                  
                                                                                                                                                                                                                                                             demo.py(179): test_model                                                     
                                                                                                                                                                                                                                                                                                                                          
                                           aten::linear         0.02%       1.506ms         0.35%      29.083ms      84.299us       0.000us         0.00%      13.000ms      37.681us           0 b           0 b     172.50 Kb       1.00 Kb           345  <built-in function linear>                                                   
                                                                                                                                                                                                                                                             torch/nn/modules/linear.py(113): forward                                     
                                                                                                                                                                                                                                                             nn.Module: Linear_78                                                         
                                                                                                                                                                                                                                                             faceformer.py(209): predict                                                  
                                                                                                                                                                                                                                                             demo.py(179): test_model                                                     
                                                                                                                                                                                                                                                                                                                                          
                                           aten::linear         0.02%       1.650ms         0.30%      24.744ms      71.722us       0.000us         0.00%      11.822ms      34.267us           0 b           0 b      19.88 Mb           0 b           345  <built-in function linear>                                                   
                                                                                                                                                                                                                                                             torch/nn/modules/linear.py(113): forward                                     
                                                                                                                                                                                                                                                             nn.Module: Linear_77                                                         
                                                                                                                                                                                                                                                             faceformer.py(209): predict                                                  
                                                                                                                                                                                                                                                             demo.py(179): test_model                                                     
                                                                                                                                                                                                                                                                                                                                          
                                            aten::addmm         0.17%      13.968ms         0.25%      20.870ms      60.493us      11.634ms         4.66%      11.822ms      34.267us           0 b           0 b      19.88 Mb    -325.12 Mb           345  <built-in function linear>                                                   
                                                                                                                                                                                                                                                             torch/nn/modules/linear.py(113): forward                                     
                                                                                                                                                                                                                                                             nn.Module: Linear_77                                                         
                                                                                                                                                                                                                                                             faceformer.py(209): predict                                                  
                                                                                                                                                                                                                                                             demo.py(179): test_model                                                     
                                                                                                                                                                                                                                                                                                                                          
void gemv2T_kernel_val<long, long, float, float, flo...         0.00%       0.000us         0.00%       0.000us       0.000us      11.634ms         4.66%      11.634ms      33.722us           0 b           0 b           0 b           0 b           345  <built-in function linear>                                                   
                                                                                                                                                                                                                                                             torch/nn/modules/linear.py(113): forward                                     
                                                                                                                                                                                                                                                             nn.Module: Linear_77                                                         
                                                                                                                                                                                                                                                             faceformer.py(209): predict                                                  
                                                                                                                                                                                                                                                             demo.py(179): test_model                                                     
                                                                                                                                                                                                                                                                                                                                          
                                           aten::linear         0.08%       6.259ms         0.64%      52.722ms      76.409us       0.000us         0.00%      11.548ms      16.736us           0 b           0 b      72.73 Mb           0 b           690  <built-in function linear>                                                   
                                                                                                                                                                                                                                                             torch/nn/functional.py(4727): _in_projection_packed                          
                                                                                                                                                                                                                                                             torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                                                                                                                             torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                                                                                                                             nn.Module: MultiheadAttention_1                                              
                                                                                                                                                                                                                                                                                                                                          
                                            aten::addmm         0.34%      28.160ms         0.48%      40.157ms      58.199us      10.778ms         4.31%      11.548ms      16.736us           0 b           0 b      72.73 Mb    -617.27 Mb           690  <built-in function linear>                                                   
                                                                                                                                                                                                                                                             torch/nn/functional.py(4727): _in_projection_packed                          
                                                                                                                                                                                                                                                             torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                                                                                                                             torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                                                                                                                             nn.Module: MultiheadAttention_1                                              
                                                                                                                                                                                                                                                                                                                                          
                                  volta_sgemm_32x128_nn         0.00%       0.000us         0.00%       0.000us       0.000us      11.061ms         4.43%      11.061ms      33.620us           0 b           0 b           0 b           0 b           329  ...in method _native_multi_head_attention of type object at 0x782330ca4880>  
                                                                                                                                                                                                                                                             torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                                                                                                                             nn.Module: MultiheadAttention_0                                              
                                                                                                                                                                                                                                                             torch/nn/modules/transformer.py(723): _sa_block                              
                                                                                                                                                                                                                                                             faceformer.py(71): quantize_decoder_forward                                  
                                                                                                                                                                                                                                                                                                                                          
                                           aten::conv1d         0.00%      10.000us         0.06%       5.317ms       5.317ms       0.000us         0.00%      10.692ms      10.692ms           0 b           0 b      35.99 Mb           0 b             1  <built-in method conv1d of type object at 0x782330ca4880>                    
                                                                                                                                                                                                                                                             torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                                                                                                                             torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                                                                                                                             nn.Module: Conv1d_1                                                          
                                                                                                                                                                                                                                                             transformers/models/wav2vec2/modeling_wav2vec2.py(311): forward              
                                                                                                                                                                                                                                                                                                                                          
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------------------------------------------  
Self CPU time total: 8.280s
Self CUDA time total: 249.830ms

Calculating MSE...

--------------------- FLOAT 16------------------------

-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------------------------------------------  
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  Source Location                                                              
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------------------------------------------  
                     aten::scaled_dot_product_attention         0.18%       3.908ms         8.17%     172.636ms     500.394us       0.000us         0.00%      44.421ms     128.757us           0 b           0 b      57.76 Mb    -108.26 Mb           345  <built-in function scaled_dot_product_attention>                             
                                                                                                                                                                                                                                                             torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                                                                                                                             torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                                                                                                                             nn.Module: MultiheadAttention_1                                              
                                                                                                                                                                                                                                                             torch/nn/modules/transformer.py(733): _mha_block                             
                                                                                                                                                                                                                                                                                                                                          
               aten::_scaled_dot_product_attention_math         0.44%       9.379ms         7.98%     168.728ms     489.067us       0.000us         0.00%      44.421ms     128.757us           0 b        -952 b     166.02 Mb    -178.83 Mb           345  <built-in function scaled_dot_product_attention>                             
                                                                                                                                                                                                                                                             torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                                                                                                                             torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                                                                                                                             nn.Module: MultiheadAttention_1                                              
                                                                                                                                                                                                                                                             torch/nn/modules/transformer.py(733): _mha_block                             
                                                                                                                                                                                                                                                                                                                                          
                     aten::_native_multi_head_attention         1.93%      40.753ms        12.11%     255.998ms     742.023us       0.000us         0.00%      36.810ms     106.696us           0 b           0 b       7.35 Mb    -316.41 Mb           345  ...in method _native_multi_head_attention of type object at 0x7ed2c12a4880>  
                                                                                                                                                                                                                                                             torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                                                                                                                             nn.Module: MultiheadAttention_0                                              
                                                                                                                                                                                                                                                             torch/nn/modules/transformer.py(723): _sa_block                              
                                                                                                                                                                                                                                                             faceformer.py(71): quantize_decoder_forward                                  
                                                                                                                                                                                                                                                                                                                                          
                                           aten::matmul         0.62%      13.178ms         4.94%     104.364ms     151.252us       0.000us         0.00%      31.179ms      45.187us           0 b           0 b     164.53 Mb           0 b           690  <built-in function scaled_dot_product_attention>                             
                                                                                                                                                                                                                                                             torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                                                                                                                             torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                                                                                                                             nn.Module: MultiheadAttention_1                                              
                                                                                                                                                                                                                                                             torch/nn/modules/transformer.py(733): _mha_block                             
                                                                                                                                                                                                                                                                                                                                          
                                              aten::bmm         2.79%      59.071ms         3.51%      74.195ms     107.529us      26.210ms        14.05%      31.179ms      45.187us           0 b           0 b     164.53 Mb     164.53 Mb           690  <built-in function scaled_dot_product_attention>                             
                                                                                                                                                                                                                                                             torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                                                                                                                             torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                                                                                                                             nn.Module: MultiheadAttention_1                                              
                                                                                                                                                                                                                                                             torch/nn/modules/transformer.py(733): _mha_block                             
                                                                                                                                                                                                                                                                                                                                          
void cutlass::Kernel<cutlass_75_wmma_tensorop_f16_s1...         0.00%       0.000us         0.00%       0.000us       0.000us      15.754ms         8.45%      15.754ms      47.027us           0 b           0 b           0 b           0 b           335  <built-in function scaled_dot_product_attention>                             
                                                                                                                                                                                                                                                             torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                                                                                                                             torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                                                                                                                             nn.Module: MultiheadAttention_1                                              
                                                                                                                                                                                                                                                             torch/nn/modules/transformer.py(733): _mha_block                             
                                                                                                                                                                                                                                                                                                                                          
                                              aten::bmm         2.36%      49.929ms         3.24%      68.493ms      99.265us      11.582ms         6.21%      14.994ms      21.730us           0 b           0 b     104.98 Mb     104.98 Mb           690  ...in method _native_multi_head_attention of type object at 0x7ed2c12a4880>  
                                                                                                                                                                                                                                                             torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                                                                                                                             nn.Module: MultiheadAttention_0                                              
                                                                                                                                                                                                                                                             torch/nn/modules/transformer.py(723): _sa_block                              
                                                                                                                                                                                                                                                             faceformer.py(71): quantize_decoder_forward                                  
                                                                                                                                                                                                                                                                                                                                          
                                           aten::linear         0.10%       2.082ms         1.99%      41.990ms     121.710us       0.000us         0.00%      12.533ms      36.328us           0 b           0 b       9.94 Mb           0 b           345  <built-in function linear>                                                   
                                                                                                                                                                                                                                                             torch/nn/modules/linear.py(113): forward                                     
                                                                                                                                                                                                                                                             nn.Module: Linear_77                                                         
                                                                                                                                                                                                                                                             faceformer.py(209): predict                                                  
                                                                                                                                                                                                                                                             demo.py(179): test_model                                                     
                                                                                                                                                                                                                                                                                                                                          
                                            aten::addmm         1.03%      21.698ms         1.75%      36.960ms     107.130us      12.234ms         6.56%      12.533ms      36.328us           0 b           0 b       9.94 Mb       8.94 Mb           345  <built-in function linear>                                                   
                                                                                                                                                                                                                                                             torch/nn/modules/linear.py(113): forward                                     
                                                                                                                                                                                                                                                             nn.Module: Linear_77                                                         
                                                                                                                                                                                                                                                             faceformer.py(209): predict                                                  
                                                                                                                                                                                                                                                             demo.py(179): test_model                                                     
                                                                                                                                                                                                                                                                                                                                          
                                           aten::linear         0.09%       1.868ms         2.07%      43.752ms     126.817us       0.000us         0.00%      10.643ms      30.849us           0 b           0 b     172.50 Kb           0 b           345  <built-in function linear>                                                   
                                                                                                                                                                                                                                                             torch/nn/modules/linear.py(113): forward                                     
                                                                                                                                                                                                                                                             nn.Module: Linear_78                                                         
                                                                                                                                                                                                                                                             faceformer.py(209): predict                                                  
                                                                                                                                                                                                                                                             demo.py(179): test_model                                                     
                                                                                                                                                                                                                                                                                                                                          
                                            aten::addmm         0.83%      17.458ms         1.78%      37.592ms     108.962us      10.149ms         5.44%      10.643ms      30.849us           0 b           0 b     172.50 Kb    -344.83 Mb           345  <built-in function linear>                                                   
                                                                                                                                                                                                                                                             torch/nn/modules/linear.py(113): forward                                     
                                                                                                                                                                                                                                                             nn.Module: Linear_78                                                         
                                                                                                                                                                                                                                                             faceformer.py(209): predict                                                  
                                                                                                                                                                                                                                                             demo.py(179): test_model                                                     
                                                                                                                                                                                                                                                                                                                                          
void gemv2T_kernel_val<long, long, __half, __half, _...         0.00%       0.000us         0.00%       0.000us       0.000us      10.477ms         5.62%      10.477ms      30.456us           0 b           0 b           0 b           0 b           344  <built-in function linear>                                                   
                                                                                                                                                                                                                                                             torch/nn/modules/linear.py(113): forward                                     
                                                                                                                                                                                                                                                             nn.Module: Linear_77                                                         
                                                                                                                                                                                                                                                             faceformer.py(209): predict                                                  
                                                                                                                                                                                                                                                             demo.py(179): test_model                                                     
                                                                                                                                                                                                                                                                                                                                          
                                         aten::_to_copy         0.15%       3.098ms         2.75%      58.042ms     168.238us       0.000us         0.00%      10.430ms      30.232us           0 b           0 b     104.98 Mb           0 b           345  <built-in method to of Tensor object at 0x7ed1a0d5fba0>                      
                                                                                                                                                                                                                                                             faceformer.py(209): predict                                                  
                                                                                                                                                                                                                                                             demo.py(179): test_model                                                     
                                                                                                                                                                                                                                                             torch/utils/_contextlib.py(115): decorate_context                            
                                                                                                                                                                                                                                                             demo.py(368): main                                                           
                                                                                                                                                                                                                                                                                                                                          
                                            aten::copy_         0.19%       4.008ms         2.41%      50.865ms     147.435us      10.166ms         5.45%      10.430ms      30.232us           0 b           0 b           0 b           0 b           345  <built-in method to of Tensor object at 0x7ed1a0d5fba0>                      
                                                                                                                                                                                                                                                             faceformer.py(209): predict                                                  
                                                                                                                                                                                                                                                             demo.py(179): test_model                                                     
                                                                                                                                                                                                                                                             torch/utils/_contextlib.py(115): decorate_context                            
                                                                                                                                                                                                                                                             demo.py(368): main                                                           
                                                                                                                                                                                                                                                                                                                                          
                                               aten::to         0.16%       3.284ms         2.89%      61.127ms     177.180us       0.000us         0.00%      10.402ms      30.151us           0 b           0 b     104.98 Mb     295.00 Kb           345  <built-in method to of Tensor object at 0x7ed1a0d5fba0>                      
                                                                                                                                                                                                                                                             faceformer.py(209): predict                                                  
                                                                                                                                                                                                                                                             demo.py(179): test_model                                                     
                                                                                                                                                                                                                                                             torch/utils/_contextlib.py(115): decorate_context                            
                                                                                                                                                                                                                                                             demo.py(368): main                                                           
                                                                                                                                                                                                                                                                                                                                          
void cutlass::Kernel<cutlass_75_wmma_tensorop_f16_s1...         0.00%       0.000us         0.00%       0.000us       0.000us      10.330ms         5.54%      10.330ms      30.029us           0 b           0 b           0 b           0 b           344  <built-in function scaled_dot_product_attention>                             
                                                                                                                                                                                                                                                             torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                                                                                                                             torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                                                                                                                             nn.Module: MultiheadAttention_1                                              
                                                                                                                                                                                                                                                             torch/nn/modules/transformer.py(733): _mha_block                             
                                                                                                                                                                                                                                                                                                                                          
                       Memcpy HtoD (Pageable -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us      10.166ms         5.45%      10.166ms      29.467us           0 b           0 b           0 b           0 b           345  <built-in method to of Tensor object at 0x7ed1a0d5fba0>                      
                                                                                                                                                                                                                                                             faceformer.py(209): predict                                                  
                                                                                                                                                                                                                                                             demo.py(179): test_model                                                     
                                                                                                                                                                                                                                                             torch/utils/_contextlib.py(115): decorate_context                            
                                                                                                                                                                                                                                                             demo.py(368): main                                                           
                                                                                                                                                                                                                                                                                                                                          
                                           aten::conv1d         0.00%       9.000us         0.30%       6.408ms       6.408ms       0.000us         0.00%       8.850ms       8.850ms           0 b           0 b     519.00 Kb           0 b             1  <built-in method conv1d of type object at 0x7ed2c12a4880>                    
                                                                                                                                                                                                                                                             torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                                                                                                                             torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                                                                                                                             nn.Module: Conv1d_7                                                          
                                                                                                                                                                                                                                                             transformers/models/wav2vec2/modeling_wav2vec2.py(396): forward              
                                                                                                                                                                                                                                                                                                                                          
                                      aten::convolution         0.00%      12.000us         0.30%       6.399ms       6.399ms       0.000us         0.00%       8.850ms       8.850ms           0 b           0 b     519.00 Kb    -517.50 Kb             1  <built-in method conv1d of type object at 0x7ed2c12a4880>                    
                                                                                                                                                                                                                                                             torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                                                                                                                             torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                                                                                                                             nn.Module: Conv1d_7                                                          
                                                                                                                                                                                                                                                             transformers/models/wav2vec2/modeling_wav2vec2.py(396): forward              
                                                                                                                                                                                                                                                                                                                                          
                                     aten::_convolution         0.00%      62.000us         0.30%       6.387ms       6.387ms       0.000us         0.00%       8.850ms       8.850ms           0 b           0 b       1.01 Mb           0 b             1  <built-in method conv1d of type object at 0x7ed2c12a4880>                    
                                                                                                                                                                                                                                                             torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                                                                                                                             torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                                                                                                                             nn.Module: Conv1d_7                                                          
                                                                                                                                                                                                                                                             transformers/models/wav2vec2/modeling_wav2vec2.py(396): forward              
                                                                                                                                                                                                                                                                                                                                          

-------------------------- LLMint8 ------------------------------

-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------------------------------------------  
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  Source Location                                                              
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------------------------------------------  
                                           MatMul8bitLt         8.22%     205.309ms        13.78%     344.475ms     998.478us     171.201ms        42.90%     190.472ms     552.093us           0 b           0 b       1.09 Mb     -37.06 Mb           345  <built-in method apply of FunctionMeta object at 0x5b2783e4cd90>             
                                                                                                                                                                                                                                                             torch/autograd/function.py(501): apply                                       
                                                                                                                                                                                                                                                             bitsandbytes/autograd/_functions.py(552): matmul                             
                                                                                                                                                                                                                                                             bitsandbytes/nn/modules.py(432): forward                                     
                                                                                                                                                                                                                                                             nn.Module: Linear8bitLt_76                                                   
                                                                                                                                                                                                                                                                                                                                          
turing_int32_i8816gemm_int8_128x128_ldg16_stages_64x...         0.00%       0.000us         0.00%       0.000us       0.000us     159.432ms        39.95%     159.432ms     462.122us           0 b           0 b           0 b           0 b           345  <built-in method apply of FunctionMeta object at 0x5b2783e4cd90>             
                                                                                                                                                                                                                                                             torch/autograd/function.py(501): apply                                       
                                                                                                                                                                                                                                                             bitsandbytes/autograd/_functions.py(552): matmul                             
                                                                                                                                                                                                                                                             bitsandbytes/nn/modules.py(432): forward                                     
                                                                                                                                                                                                                                                             nn.Module: Linear8bitLt_76                                                   
                                                                                                                                                                                                                                                                                                                                          
                     aten::scaled_dot_product_attention         0.13%       3.140ms         5.86%     146.467ms     424.542us       0.000us         0.00%      48.818ms     141.501us           0 b           0 b      51.42 Mb    -114.18 Mb           345  <built-in function scaled_dot_product_attention>                             
                                                                                                                                                                                                                                                             torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                                                                                                                             torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                                                                                                                             nn.Module: MultiheadAttention_1                                              
                                                                                                                                                                                                                                                             torch/nn/modules/transformer.py(733): _mha_block                             
                                                                                                                                                                                                                                                                                                                                          
               aten::_scaled_dot_product_attention_math         0.32%       7.897ms         5.74%     143.327ms     415.441us       0.000us         0.00%      48.818ms     141.501us           0 b      -1.07 Kb     165.60 Mb    -179.93 Mb           345  <built-in function scaled_dot_product_attention>                             
                                                                                                                                                                                                                                                             torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                                                                                                                             torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                                                                                                                             nn.Module: MultiheadAttention_1                                              
                                                                                                                                                                                                                                                             torch/nn/modules/transformer.py(733): _mha_block                             
                                                                                                                                                                                                                                                                                                                                          
                                           MatMul8bitLt         9.03%     225.666ms        15.06%     376.297ms       1.091ms      25.117ms         6.29%      44.282ms     128.354us           0 b           0 b      10.86 Mb     -40.94 Mb           345  <built-in method apply of FunctionMeta object at 0x5b2783e4cd90>             
                                                                                                                                                                                                                                                             torch/autograd/function.py(501): apply                                       
                                                                                                                                                                                                                                                             bitsandbytes/autograd/_functions.py(552): matmul                             
                                                                                                                                                                                                                                                             bitsandbytes/nn/modules.py(432): forward                                     
                                                                                                                                                                                                                                                             nn.Module: Linear8bitLt_75                                                   
                                                                                                                                                                                                                                                                                                                                          
                     aten::_native_multi_head_attention         1.37%      34.172ms         8.77%     219.177ms     635.296us       0.000us         0.00%      41.626ms     120.655us           0 b           0 b       7.35 Mb    -316.90 Mb           345  ...in method _native_multi_head_attention of type object at 0x7b95044a4880>  
                                                                                                                                                                                                                                                             torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                                                                                                                             nn.Module: MultiheadAttention_0                                              
                                                                                                                                                                                                                                                             torch/nn/modules/transformer.py(723): _sa_block                              
                                                                                                                                                                                                                                                             faceformer.py(71): quantize_decoder_forward                                  
                                                                                                                                                                                                                                                                                                                                          
                                           aten::matmul         0.55%      13.630ms         3.67%      91.772ms     133.003us       0.000us         0.00%      34.979ms      50.694us           0 b           0 b     164.53 Mb           0 b           690  <built-in function scaled_dot_product_attention>                             
                                                                                                                                                                                                                                                             torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                                                                                                                             torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                                                                                                                             nn.Module: MultiheadAttention_1                                              
                                                                                                                                                                                                                                                             torch/nn/modules/transformer.py(733): _mha_block                             
                                                                                                                                                                                                                                                                                                                                          
                                              aten::bmm         2.07%      51.796ms         2.54%      63.586ms      92.154us      26.095ms         6.54%      34.979ms      50.694us           0 b           0 b     164.53 Mb     164.53 Mb           690  <built-in function scaled_dot_product_attention>                             
                                                                                                                                                                                                                                                             torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                                                                                                                             torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                                                                                                                             nn.Module: MultiheadAttention_1                                              
                                                                                                                                                                                                                                                             torch/nn/modules/transformer.py(733): _mha_block                             
                                                                                                                                                                                                                                                                                                                                          
                                              aten::bmm         1.61%      40.150ms         2.33%      58.196ms      84.342us      11.548ms         2.89%      19.189ms      27.810us           0 b           0 b     104.98 Mb     104.98 Mb           690  ...in method _native_multi_head_attention of type object at 0x7b95044a4880>  
                                                                                                                                                                                                                                                             torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                                                                                                                             nn.Module: MultiheadAttention_0                                              
                                                                                                                                                                                                                                                             torch/nn/modules/transformer.py(723): _sa_block                              
                                                                                                                                                                                                                                                             faceformer.py(71): quantize_decoder_forward                                  
                                                                                                                                                                                                                                                                                                                                          
                                          aten::cumsum_         1.13%      28.263ms         1.91%      47.662ms      62.303us       7.143ms         1.79%      16.346ms      21.367us           0 b           0 b           0 b           0 b           765  <built-in method cumsum_ of Tensor object at 0x7b93e4464770>                 
                                                                                                                                                                                                                                                             bitsandbytes/functional.py(1850): get_colrow_absmax                          
                                                                                                                                                                                                                                                             bitsandbytes/functional.py(1983): double_quant                               
                                                                                                                                                                                                                                                             bitsandbytes/autograd/_functions.py(295): forward                            
                                                                                                                                                                                                                                                             <built-in method apply of FunctionMeta object at 0x5b2783e4cd90>             
                                                                                                                                                                                                                                                                                                                                          
void cutlass::Kernel<cutlass_75_wmma_tensorop_f16_s1...         0.00%       0.000us         0.00%       0.000us       0.000us      15.733ms         3.94%      15.733ms      46.964us           0 b           0 b           0 b           0 b           335  <built-in function scaled_dot_product_attention>                             
                                                                                                                                                                                                                                                             torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                                                                                                                             torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                                                                                                                             nn.Module: MultiheadAttention_1                                              
                                                                                                                                                                                                                                                             torch/nn/modules/transformer.py(733): _mha_block                             
                                                                                                                                                                                                                                                                                                                                          
                                         aten::_to_copy         0.11%       2.868ms         2.27%      56.763ms     164.530us       0.000us         0.00%      11.574ms      33.548us           0 b           0 b     104.98 Mb           0 b           345  <built-in method to of Tensor object at 0x7b93e1792ed0>                      
                                                                                                                                                                                                                                                             faceformer.py(209): predict                                                  
                                                                                                                                                                                                                                                             demo.py(179): test_model                                                     
                                                                                                                                                                                                                                                             torch/utils/_contextlib.py(115): decorate_context                            
                                                                                                                                                                                                                                                             demo.py(368): main                                                           
                                                                                                                                                                                                                                                                                                                                          
                                            aten::copy_         0.14%       3.558ms         2.01%      50.285ms     145.754us      10.430ms         2.61%      11.574ms      33.548us           0 b           0 b           0 b           0 b           345  <built-in method to of Tensor object at 0x7b93e1792ed0>                      
                                                                                                                                                                                                                                                             faceformer.py(209): predict                                                  
                                                                                                                                                                                                                                                             demo.py(179): test_model                                                     
                                                                                                                                                                                                                                                             torch/utils/_contextlib.py(115): decorate_context                            
                                                                                                                                                                                                                                                             demo.py(368): main                                                           
                                                                                                                                                                                                                                                                                                                                          
                                               aten::to         0.15%       3.862ms         2.38%      59.487ms     172.426us       0.000us         0.00%      11.403ms      33.052us           0 b           0 b     104.98 Mb       1.60 Mb           345  <built-in method to of Tensor object at 0x7b93e1792ed0>                      
                                                                                                                                                                                                                                                             faceformer.py(209): predict                                                  
                                                                                                                                                                                                                                                             demo.py(179): test_model                                                     
                                                                                                                                                                                                                                                             torch/utils/_contextlib.py(115): decorate_context                            
                                                                                                                                                                                                                                                             demo.py(368): main                                                           
                                                                                                                                                                                                                                                                                                                                          
void kdequant_mm_int32_fp16<4, 128, 512>(int*, float...         0.00%       0.000us         0.00%       0.000us       0.000us      10.782ms         2.70%      10.782ms      31.252us           0 b           0 b           0 b           0 b           345  <built-in method apply of FunctionMeta object at 0x5b2783e4cd90>             
                                                                                                                                                                                                                                                             torch/autograd/function.py(501): apply                                       
                                                                                                                                                                                                                                                             bitsandbytes/autograd/_functions.py(552): matmul                             
                                                                                                                                                                                                                                                             bitsandbytes/nn/modules.py(432): forward                                     
                                                                                                                                                                                                                                                             nn.Module: Linear8bitLt_75                                                   
                                                                                                                                                                                                                                                                                                                                          
                       Memcpy HtoD (Pageable -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us      10.430ms         2.61%      10.430ms      30.232us           0 b           0 b           0 b           0 b           345  <built-in method to of Tensor object at 0x7b93e1792ed0>                      
                                                                                                                                                                                                                                                             faceformer.py(209): predict                                                  
                                                                                                                                                                                                                                                             demo.py(179): test_model                                                     
                                                                                                                                                                                                                                                             torch/utils/_contextlib.py(115): decorate_context                            
                                                                                                                                                                                                                                                             demo.py(368): main                                                           
                                                                                                                                                                                                                                                                                                                                          
void cutlass::Kernel<cutlass_75_wmma_tensorop_f16_s1...         0.00%       0.000us         0.00%       0.000us       0.000us      10.237ms         2.57%      10.237ms      29.759us           0 b           0 b           0 b           0 b           344  <built-in function scaled_dot_product_attention>                             
                                                                                                                                                                                                                                                             torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                                                                                                                             torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                                                                                                                             nn.Module: MultiheadAttention_1                                              
                                                                                                                                                                                                                                                             torch/nn/modules/transformer.py(733): _mha_block                             
                                                                                                                                                                                                                                                                                                                                          
                                           aten::conv1d         0.00%       8.000us         0.18%       4.455ms       4.455ms       0.000us         0.00%       9.941ms       9.941ms           0 b           0 b     519.00 Kb           0 b             1  <built-in method conv1d of type object at 0x7b95044a4880>                    
                                                                                                                                                                                                                                                             torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                                                                                                                             torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                                                                                                                             nn.Module: Conv1d_7                                                          
                                                                                                                                                                                                                                                             transformers/models/wav2vec2/modeling_wav2vec2.py(396): forward              
                                                                                                                                                                                                                                                                                                                                          
                                      aten::convolution         0.00%       9.000us         0.18%       4.447ms       4.447ms       0.000us         0.00%       9.941ms       9.941ms           0 b           0 b     519.00 Kb           0 b             1  <built-in method conv1d of type object at 0x7b95044a4880>                    
                                                                                                                                                                                                                                                             torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                                                                                                                             torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                                                                                                                             nn.Module: Conv1d_7                                                          
                                                                                                                                                                                                                                                             transformers/models/wav2vec2/modeling_wav2vec2.py(396): forward              
                                                                                                                                                                                                                                                                                                                                          
                                     aten::_convolution         0.00%      52.000us         0.18%       4.438ms       4.438ms       0.000us         0.00%       9.941ms       9.941ms           0 b           0 b     519.00 Kb    -517.50 Kb             1  <built-in method conv1d of type object at 0x7b95044a4880>                    
                                                                                                                                                                                                                                                             torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                                                                                                                             torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                                                                                                                             nn.Module: Conv1d_7                                                          
                                                                                                                                                                                                                                                             transformers/models/wav2vec2/modeling_wav2vec2.py(396): forward              
                                                                                                                                                                                                                                                                                                                                          
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  --------------------------------------------------------------------------- 