----------------------------- REGULAR RUN-----------------------


--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------------------------------------------  
                                        Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  Source Location                                                              
--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------------------------------------------  
                                aten::linear         0.06%      18.785ms        74.64%       22.979s      80.066ms      10.79 Gb           0 b           287  <built-in function linear>                                                   
                                                                                                                                                              torch/nn/modules/linear.py(113): forward                                     
                                                                                                                                                              nn.Module: Linear_77                                                         
                                                                                                                                                              faceformer.py(209): predict                                                  
                                                                                                                                                              demo.py(133): test_model                                                     
                                                                                                                                                                                                                                           
                                 aten::addmm        50.34%       15.495s        74.54%       22.946s      79.952ms      10.79 Gb      10.79 Gb           287  <built-in function linear>                                                   
                                                                                                                                                              torch/nn/modules/linear.py(113): forward                                     
                                                                                                                                                              nn.Module: Linear_77                                                         
                                                                                                                                                              faceformer.py(209): predict                                                  
                                                                                                                                                              demo.py(133): test_model                                                     
                                                                                                                                                                                                                                           
                                 aten::copy_        24.20%        7.449s        24.20%        7.449s      25.955ms           0 b           0 b           287  <built-in function linear>                                                   
                                                                                                                                                              torch/nn/modules/linear.py(113): forward                                     
                                                                                                                                                              nn.Module: Linear_77                                                         
                                                                                                                                                              faceformer.py(209): predict                                                  
                                                                                                                                                              demo.py(133): test_model                                                     
                                                                                                                                                                                                                                           
                                aten::linear         0.02%       5.795ms         3.32%        1.021s       3.558ms     143.50 Kb           0 b           287  <built-in function linear>                                                   
                                                                                                                                                              torch/nn/modules/linear.py(113): forward                                     
                                                                                                                                                              nn.Module: Linear_78                                                         
                                                                                                                                                              faceformer.py(209): predict                                                  
                                                                                                                                                              demo.py(133): test_model                                                     
                                                                                                                                                                                                                                           
                                 aten::addmm         3.26%        1.004s         3.28%        1.011s       3.521ms     143.50 Kb     143.50 Kb           287  <built-in function linear>                                                   
                                                                                                                                                              torch/nn/modules/linear.py(113): forward                                     
                                                                                                                                                              nn.Module: Linear_78                                                         
                                                                                                                                                              faceformer.py(209): predict                                                  
                                                                                                                                                              demo.py(133): test_model                                                     
                                                                                                                                                                                                                                           
          aten::scaled_dot_product_attention         0.01%       2.631ms         2.32%     714.431ms       2.489ms      27.50 Mb    -354.83 Mb           287  <built-in function scaled_dot_product_attention>                             
                                                                                                                                                              torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_1                                              
                                                                                                                                                              torch/nn/modules/transformer.py(733): _mha_block                             
                                                                                                                                                                                                                                           
    aten::_scaled_dot_product_attention_math         0.04%      12.893ms         2.31%     711.800ms       2.480ms     382.33 Mb    -461.95 Mb           287  <built-in function scaled_dot_product_attention>                             
                                                                                                                                                              torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_1                                              
                                                                                                                                                              torch/nn/modules/transformer.py(733): _mha_block                             
                                                                                                                                                                                                                                           
          aten::_native_multi_head_attention         0.12%      37.975ms         2.13%     655.891ms       2.285ms      20.18 Mb    -414.70 Mb           287  ...in method _native_multi_head_attention of type object at 0x7e234d4a4880>  
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_0                                              
                                                                                                                                                              torch/nn/modules/transformer.py(723): _sa_block                              
                                                                                                                                                              faceformer.py(71): quantize_decoder_forward                                  
                                                                                                                                                                                                                                           
                                aten::conv1d         0.00%      20.000us         1.80%     554.787ms     554.787ms      35.99 Mb           0 b             1  <built-in method conv1d of type object at 0x7e234d4a4880>                    
                                                                                                                                                              torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                              torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                              nn.Module: Conv1d_1                                                          
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(311): forward              
                                                                                                                                                                                                                                           
                           aten::convolution         0.00%      26.000us         1.80%     554.767ms     554.767ms      35.99 Mb           0 b             1  <built-in method conv1d of type object at 0x7e234d4a4880>                    
                                                                                                                                                              torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                              torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                              nn.Module: Conv1d_1                                                          
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(311): forward              
                                                                                                                                                                                                                                           
                          aten::_convolution         0.00%      61.000us         1.80%     554.741ms     554.741ms      35.99 Mb           0 b             1  <built-in method conv1d of type object at 0x7e234d4a4880>                    
                                                                                                                                                              torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                              torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                              nn.Module: Conv1d_1                                                          
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(311): forward              
                                                                                                                                                                                                                                           
                    aten::mkldnn_convolution         1.80%     554.558ms         1.80%     554.636ms     554.636ms      35.99 Mb           0 b             1  <built-in method conv1d of type object at 0x7e234d4a4880>                    
                                                                                                                                                              torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                              torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                              nn.Module: Conv1d_1                                                          
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(311): forward              
                                                                                                                                                                                                                                           
                                aten::matmul         0.05%      15.616ms         1.16%     355.733ms     619.744us     381.59 Mb    -574.00 Kb           574  <built-in function scaled_dot_product_attention>                             
                                                                                                                                                              torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_1                                              
                                                                                                                                                              torch/nn/modules/transformer.py(733): _mha_block                             
                                                                                                                                                                                                                                           
                                aten::linear         0.03%       9.008ms         1.06%     325.838ms     567.662us     181.06 Mb           0 b           574  <built-in function linear>                                                   
                                                                                                                                                              torch/nn/functional.py(4727): _in_projection_packed                          
                                                                                                                                                              torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_1                                              
                                                                                                                                                                                                                                           
                                   aten::bmm         1.04%     319.734ms         1.04%     319.738ms     557.035us     382.15 Mb     382.15 Mb           574  <built-in function scaled_dot_product_attention>                             
                                                                                                                                                              torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_1                                              
                                                                                                                                                              torch/nn/modules/transformer.py(733): _mha_block                             
                                                                                                                                                                                                                                           
                                 aten::addmm         0.90%     276.592ms         1.00%     307.746ms     536.143us     181.06 Mb     181.06 Mb           574  <built-in function linear>                                                   
                                                                                                                                                              torch/nn/functional.py(4727): _in_projection_packed                          
                                                                                                                                                              torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_1                                              
                                                                                                                                                                                                                                           
                                aten::conv1d         0.00%      14.000us         0.86%     265.045ms     265.045ms      17.99 Mb           0 b             1  <built-in method conv1d of type object at 0x7e234d4a4880>                    
                                                                                                                                                              torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                              torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                              nn.Module: Conv1d_2                                                          
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(311): forward              
                                                                                                                                                                                                                                           
                           aten::convolution         0.00%      21.000us         0.86%     265.031ms     265.031ms      17.99 Mb           0 b             1  <built-in method conv1d of type object at 0x7e234d4a4880>                    
                                                                                                                                                              torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                              torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                              nn.Module: Conv1d_2                                                          
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(311): forward              
                                                                                                                                                                                                                                           
                          aten::_convolution         0.00%      39.000us         0.86%     265.010ms     265.010ms      17.99 Mb           0 b             1  <built-in method conv1d of type object at 0x7e234d4a4880>                    
                                                                                                                                                              torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                              torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                              nn.Module: Conv1d_2                                                          
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(311): forward              
                                                                                                                                                                                                                                           
                    aten::mkldnn_convolution         0.86%     264.884ms         0.86%     264.938ms     264.938ms      17.99 Mb           0 b             1  <built-in method conv1d of type object at 0x7e234d4a4880>                    
                                                                                                                                                              torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                              torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                              nn.Module: Conv1d_2                                                          
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(311): forward              
                                                                                                                                                                                                                                           
--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------------------------------------------  
Self CPU time total: 30.784s


----------------------------- Optimized last layer -----------------------
--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------------------------------------------  
                                        Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  Source Location                                                              
--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------------------------------------------  
                                aten::linear         0.04%       3.386ms        14.33%        1.367s       4.764ms      76.76 Mb           0 b           287  <built-in function linear>                                                   
                                                                                                                                                              torch/nn/modules/linear.py(113): forward                                     
                                                                                                                                                              nn.Module: Linear_77                                                         
                                                                                                                                                              faceformer.py(209): predict                                                  
                                                                                                                                                              demo.py(133): test_model                                                     
                                                                                                                                                                                                                                           
                                 aten::addmm        14.10%        1.345s        14.27%        1.361s       4.742ms      76.76 Mb      76.76 Mb           287  <built-in function linear>                                                   
                                                                                                                                                              torch/nn/modules/linear.py(113): forward                                     
                                                                                                                                                              nn.Module: Linear_77                                                         
                                                                                                                                                              faceformer.py(209): predict                                                  
                                                                                                                                                              demo.py(133): test_model                                                     
                                                                                                                                                                                                                                           
                                aten::linear         0.04%       4.014ms        10.48%     999.885ms       3.484ms     143.50 Kb           0 b           287  <built-in function linear>                                                   
                                                                                                                                                              torch/nn/modules/linear.py(113): forward                                     
                                                                                                                                                              nn.Module: Linear_78                                                         
                                                                                                                                                              faceformer.py(209): predict                                                  
                                                                                                                                                              demo.py(133): test_model                                                     
                                                                                                                                                                                                                                           
                                 aten::addmm        10.33%     985.377ms        10.37%     989.229ms       3.447ms     143.50 Kb     143.50 Kb           287  <built-in function linear>                                                   
                                                                                                                                                              torch/nn/modules/linear.py(113): forward                                     
                                                                                                                                                              nn.Module: Linear_78                                                         
                                                                                                                                                              faceformer.py(209): predict                                                  
                                                                                                                                                              demo.py(133): test_model                                                     
                                                                                                                                                                                                                                           
          aten::scaled_dot_product_attention         0.03%       2.534ms         8.44%     805.337ms       2.806ms      22.78 Mb    -359.45 Mb           287  <built-in function scaled_dot_product_attention>                             
                                                                                                                                                              torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_1                                              
                                                                                                                                                              torch/nn/modules/transformer.py(733): _mha_block                             
                                                                                                                                                                                                                                           
    aten::_scaled_dot_product_attention_math         0.13%      12.641ms         8.41%     802.803ms       2.797ms     382.23 Mb    -461.78 Mb           287  <built-in function scaled_dot_product_attention>                             
                                                                                                                                                              torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_1                                              
                                                                                                                                                              torch/nn/modules/transformer.py(733): _mha_block                             
                                                                                                                                                                                                                                           
          aten::_native_multi_head_attention         0.40%      38.255ms         7.95%     758.400ms       2.643ms      20.18 Mb    -414.01 Mb           287  ...in method _native_multi_head_attention of type object at 0x7f57b4ea4880>  
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_0                                              
                                                                                                                                                              torch/nn/modules/transformer.py(723): _sa_block                              
                                                                                                                                                              faceformer.py(71): quantize_decoder_forward                                  
                                                                                                                                                                                                                                           
                                aten::conv1d         0.00%      13.000us         5.72%     545.372ms     545.372ms      35.99 Mb           0 b             1  <built-in method conv1d of type object at 0x7f57b4ea4880>                    
                                                                                                                                                              torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                              torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                              nn.Module: Conv1d_1                                                          
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(311): forward              
                                                                                                                                                                                                                                           
                           aten::convolution         0.00%      18.000us         5.72%     545.359ms     545.359ms      35.99 Mb           0 b             1  <built-in method conv1d of type object at 0x7f57b4ea4880>                    
                                                                                                                                                              torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                              torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                              nn.Module: Conv1d_1                                                          
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(311): forward              
                                                                                                                                                                                                                                           
                          aten::_convolution         0.00%      44.000us         5.72%     545.341ms     545.341ms      35.99 Mb           0 b             1  <built-in method conv1d of type object at 0x7f57b4ea4880>                    
                                                                                                                                                              torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                              torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                              nn.Module: Conv1d_1                                                          
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(311): forward              
                                                                                                                                                                                                                                           
                    aten::mkldnn_convolution         5.71%     545.197ms         5.72%     545.263ms     545.263ms      35.99 Mb           0 b             1  <built-in method conv1d of type object at 0x7f57b4ea4880>                    
                                                                                                                                                              torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                              torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                              nn.Module: Conv1d_1                                                          
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(311): forward              
                                                                                                                                                                                                                                           
                                aten::matmul         0.17%      16.267ms         4.32%     411.681ms     717.214us     381.59 Mb    -574.00 Kb           574  <built-in function scaled_dot_product_attention>                             
                                                                                                                                                              torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_1                                              
                                                                                                                                                              torch/nn/modules/transformer.py(733): _mha_block                             
                                                                                                                                                                                                                                           
                                   aten::bmm         3.91%     373.176ms         3.91%     373.236ms     650.237us     382.15 Mb     382.15 Mb           574  <built-in function scaled_dot_product_attention>                             
                                                                                                                                                              torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_1                                              
                                                                                                                                                              torch/nn/modules/transformer.py(733): _mha_block                             
                                                                                                                                                                                                                                           
                                aten::linear         0.10%       9.187ms         3.72%     354.499ms     617.594us     181.06 Mb           0 b           574  <built-in function linear>                                                   
                                                                                                                                                              torch/nn/functional.py(4727): _in_projection_packed                          
                                                                                                                                                              torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_1                                              
                                                                                                                                                                                                                                           
                                 aten::addmm         3.23%     308.477ms         3.52%     335.649ms     584.754us     181.06 Mb     181.06 Mb           574  <built-in function linear>                                                   
                                                                                                                                                              torch/nn/functional.py(4727): _in_projection_packed                          
                                                                                                                                                              torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_1                                              
                                                                                                                                                                                                                                           
                               aten::softmax         0.02%       2.334ms         2.95%     281.531ms     980.944us     361.26 Mb    -466.38 Kb           287  <built-in function scaled_dot_product_attention>                             
                                                                                                                                                              torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_1                                              
                                                                                                                                                              torch/nn/modules/transformer.py(733): _mha_block                             
                                                                                                                                                                                                                                           
                              aten::_softmax         2.93%     279.328ms         2.93%     279.328ms     973.268us     361.97 Mb     361.97 Mb           287  <built-in function scaled_dot_product_attention>                             
                                                                                                                                                              torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_1                                              
                                                                                                                                                              torch/nn/modules/transformer.py(733): _mha_block                             
                                                                                                                                                                                                                                           
                       aten::_masked_softmax         2.82%     269.292ms         2.85%     272.164ms     948.307us     120.74 Mb    -199.20 Kb           287  ...in method _native_multi_head_attention of type object at 0x7f57b4ea4880>  
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_0                                              
                                                                                                                                                              torch/nn/modules/transformer.py(723): _sa_block                              
                                                                                                                                                              faceformer.py(71): quantize_decoder_forward                                  
                                                                                                                                                                                                                                           
                                aten::conv1d         0.00%      15.000us         2.79%     266.017ms     266.017ms      17.99 Mb           0 b             1  <built-in method conv1d of type object at 0x7f57b4ea4880>                    
                                                                                                                                                              torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                              torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                              nn.Module: Conv1d_2                                                          
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(311): forward              
                                                                                                                                                                                                                                           
                           aten::convolution         0.00%      17.000us         2.79%     266.002ms     266.002ms      17.99 Mb           0 b             1  <built-in method conv1d of type object at 0x7f57b4ea4880>                    
                                                                                                                                                              torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                              torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                              nn.Module: Conv1d_2                                                          
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(311): forward              
                                                                                                                                                                                                                                           
--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------------------------------------------  
Self CPU time total: 9.540s


----------------------------- Optimized last layer + static Quantization (vertice_map, vertice_map_r quantized)-----------------------

--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------------------------------------------  
                                        Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  Source Location                                                              
--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------------------------------------------  
          aten::scaled_dot_product_attention         0.04%       2.787ms        10.34%     798.982ms       2.784ms      23.71 Mb    -357.26 Mb           287  <built-in function scaled_dot_product_attention>                             
                                                                                                                                                              torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_1                                              
                                                                                                                                                              torch/nn/modules/transformer.py(733): _mha_block                             
                                                                                                                                                                                                                                           
    aten::_scaled_dot_product_attention_math         0.17%      13.239ms        10.31%     796.195ms       2.774ms     380.97 Mb    -461.21 Mb           287  <built-in function scaled_dot_product_attention>                             
                                                                                                                                                              torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_1                                              
                                                                                                                                                              torch/nn/modules/transformer.py(733): _mha_block                             
                                                                                                                                                                                                                                           
          aten::_native_multi_head_attention         0.53%      41.212ms         9.58%     740.397ms       2.580ms      20.18 Mb    -413.61 Mb           287  ...in method _native_multi_head_attention of type object at 0x7e67f52a4880>  
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_0                                              
                                                                                                                                                              torch/nn/modules/transformer.py(723): _sa_block                              
                                                                                                                                                              torch/nn/modules/transformer.py(681): forward                                
                                                                                                                                                                                                                                           
                           quantized::linear         8.11%     626.278ms         8.19%     632.390ms       2.203ms      19.19 Mb     -75.35 Mb           287  <built-in method linear of PyCapsule object at 0x7e677051dbf0>               
                                                                                                                                                              torch/_ops.py(497): __call__                                                 
                                                                                                                                                              torch/ao/nn/quantized/modules/linear.py(167): forward                        
                                                                                                                                                              nn.Module: Linear_0                                                          
                                                                                                                                                              faceformer.py(214): predict                                                  
                                                                                                                                                                                                                                           
                                aten::conv1d         0.00%      13.000us         7.04%     543.568ms     543.568ms      35.99 Mb           0 b             1  <built-in method conv1d of type object at 0x7e67f52a4880>                    
                                                                                                                                                              torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                              torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                              nn.Module: Conv1d_1                                                          
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(311): forward              
                                                                                                                                                                                                                                           
                           aten::convolution         0.00%      15.000us         7.04%     543.555ms     543.555ms      35.99 Mb           0 b             1  <built-in method conv1d of type object at 0x7e67f52a4880>                    
                                                                                                                                                              torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                              torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                              nn.Module: Conv1d_1                                                          
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(311): forward              
                                                                                                                                                                                                                                           
                          aten::_convolution         0.00%      49.000us         7.04%     543.540ms     543.540ms      35.99 Mb           0 b             1  <built-in method conv1d of type object at 0x7e67f52a4880>                    
                                                                                                                                                              torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                              torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                              nn.Module: Conv1d_1                                                          
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(311): forward              
                                                                                                                                                                                                                                           
                    aten::mkldnn_convolution         7.03%     543.392ms         7.03%     543.455ms     543.455ms      35.99 Mb           0 b             1  <built-in method conv1d of type object at 0x7e67f52a4880>                    
                                                                                                                                                              torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                              torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                              nn.Module: Conv1d_1                                                          
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(311): forward              
                                                                                                                                                                                                                                           
                                aten::matmul         0.21%      16.558ms         5.20%     401.807ms     700.012us     380.43 Mb      -1.72 Mb           574  <built-in function scaled_dot_product_attention>                             
                                                                                                                                                              torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_1                                              
                                                                                                                                                              torch/nn/modules/transformer.py(733): _mha_block                             
                                                                                                                                                                                                                                           
                                   aten::bmm         4.69%     362.016ms         4.69%     362.068ms     630.780us     382.15 Mb     382.15 Mb           574  <built-in function scaled_dot_product_attention>                             
                                                                                                                                                              torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_1                                              
                                                                                                                                                              torch/nn/modules/transformer.py(733): _mha_block                             
                                                                                                                                                                                                                                           
                                aten::linear         0.12%       9.396ms         4.46%     344.542ms     600.247us     181.06 Mb           0 b           574  <built-in function linear>                                                   
                                                                                                                                                              torch/nn/functional.py(4727): _in_projection_packed                          
                                                                                                                                                              torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_1                                              


----------------------------- Optimized last layer + dynamic Quantization-----------------------


--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------------------------------------------  
                                        Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  Source Location                                                              
--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------------------------------------------  
                                aten::conv1d         0.00%      14.000us        11.11%     807.087ms     807.087ms      35.99 Mb           0 b             1  <built-in method conv1d of type object at 0x7a2b604a4880>                    
                                                                                                                                                              torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                              torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                              nn.Module: Conv1d_1                                                          
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(311): forward              
                                                                                                                                                                                                                                           
                           aten::convolution         0.00%      17.000us        11.11%     807.073ms     807.073ms      35.99 Mb           0 b             1  <built-in method conv1d of type object at 0x7a2b604a4880>                    
                                                                                                                                                              torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                              torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                              nn.Module: Conv1d_1                                                          
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(311): forward              
                                                                                                                                                                                                                                           
                          aten::_convolution         0.00%      42.000us        11.11%     807.056ms     807.056ms      35.99 Mb           0 b             1  <built-in method conv1d of type object at 0x7a2b604a4880>                    
                                                                                                                                                              torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                              torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                              nn.Module: Conv1d_1                                                          
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(311): forward              
                                                                                                                                                                                                                                           
                    aten::mkldnn_convolution        11.10%     806.915ms        11.11%     806.980ms     806.980ms      35.99 Mb           0 b             1  <built-in method conv1d of type object at 0x7a2b604a4880>                    
                                                                                                                                                              torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                              torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                              nn.Module: Conv1d_1                                                          
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(311): forward              
                                                                                                                                                                                                                                           
          aten::scaled_dot_product_attention         0.03%       2.285ms         8.00%     581.558ms       2.026ms      32.39 Mb    -349.87 Mb           287  <built-in function scaled_dot_product_attention>                             
                                                                                                                                                              torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_1                                              
                                                                                                                                                              torch/nn/modules/transformer.py(733): _mha_block                             
                                                                                                                                                                                                                                           
    aten::_scaled_dot_product_attention_math         0.15%      10.890ms         7.97%     579.273ms       2.018ms     382.26 Mb    -461.13 Mb           287  <built-in function scaled_dot_product_attention>                             
                                                                                                                                                              torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_1                                              
                                                                                                                                                              torch/nn/modules/transformer.py(733): _mha_block                             
                                                                                                                                                                                                                                           
                   quantized::linear_dynamic         7.34%     533.053ms         7.38%     536.204ms       1.868ms      76.76 Mb     -77.56 Mb           287  <built-in method linear_dynamic of PyCapsule object at 0x7a2adbd7fcc0>       
                                                                                                                                                              torch/_ops.py(497): __call__                                                 
                                                                                                                                                              torch/ao/nn/quantized/dynamic/modules/linear.py(47): forward                 
                                                                                                                                                              nn.Module: Linear_77                                                         
                                                                                                                                                              faceformer.py(209): predict                                                  
                                                                                                                                                                                                                                           
          aten::_native_multi_head_attention         0.39%      28.635ms         7.21%     523.653ms       1.825ms      20.18 Mb    -415.60 Mb           287  ...in method _native_multi_head_attention of type object at 0x7a2b604a4880>  
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_0                                              
                                                                                                                                                              torch/nn/modules/transformer.py(723): _sa_block                              
                                                                                                                                                              faceformer.py(71): quantize_decoder_forward                                  
                                                                                                                                                                                                                                           
                                aten::conv1d         0.00%      16.000us         5.48%     398.145ms     398.145ms      17.99 Mb           0 b             1  <built-in method conv1d of type object at 0x7a2b604a4880>                    
                                                                                                                                                              torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                              torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                              nn.Module: Conv1d_2                                                          
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(311): forward              
                                                                                                                                                                                                                                           
                           aten::convolution         0.00%      17.000us         5.48%     398.129ms     398.129ms      17.99 Mb           0 b             1  <built-in method conv1d of type object at 0x7a2b604a4880>                    
                                                                                                                                                              torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                              torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                              nn.Module: Conv1d_2                                                          
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(311): forward              
                                                                                                                                                                                                                                           
                          aten::_convolution         0.00%      46.000us         5.48%     398.112ms     398.112ms      17.99 Mb           0 b             1  <built-in method conv1d of type object at 0x7a2b604a4880>                    
                                                                                                                                                              torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                              torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                              nn.Module: Conv1d_2                                                          
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(311): forward              
                                                                                                                                                                                                                                           
                    aten::mkldnn_convolution         5.48%     397.973ms         5.48%     398.027ms     398.027ms      17.99 Mb           0 b             1  <built-in method conv1d of type object at 0x7a2b604a4880>                    
                                                                                                                                                              torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                              torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                              nn.Module: Conv1d_2                                                          
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(311): forward              
                                                                                                                                                                                                                                           
                   quantized::linear_dynamic         3.92%     284.537ms         3.97%     288.322ms       1.005ms     143.50 Kb    -143.50 Kb           287  <built-in method linear_dynamic of PyCapsule object at 0x7a2adbd7fcc0>       
                                                                                                                                                              torch/_ops.py(497): __call__                                                 
                                                                                                                                                              torch/ao/nn/quantized/dynamic/modules/linear.py(47): forward                 
                                                                                                                                                              nn.Module: Linear_78                                                         
                                                                                                                                                              faceformer.py(209): predict                                                  
                                                                                                                                                                                                                                           
                                aten::conv1d         0.00%      13.000us         3.84%     278.681ms     278.681ms       1.68 Mb           0 b             1  <built-in method conv1d of type object at 0x7a2b604a4880>                    
                                                                                                                                                              torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                              torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                              nn.Module: Conv1d_7                                                          
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(396): forward              
                                                                                                                                                                                                                                           
                           aten::convolution         0.00%      23.000us         3.83%     278.668ms     278.668ms       1.68 Mb           0 b             1  <built-in method conv1d of type object at 0x7a2b604a4880>                    
                                                                                                                                                              torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                              torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                              nn.Module: Conv1d_7                                                          
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(396): forward              
                                                                                                                                                                                                                                           
                          aten::_convolution         0.00%      64.000us         3.83%     278.645ms     278.645ms       1.68 Mb      -1.68 Mb             1  <built-in method conv1d of type object at 0x7a2b604a4880>                    
                                                                                                                                                              torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                              torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                              nn.Module: Conv1d_7                                                          
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(396): forward              
                                                                                                                                                                                                                                           
                                aten::matmul         0.17%      12.247ms         3.82%     277.639ms     483.692us     381.30 Mb    -876.50 Kb           574  <built-in function scaled_dot_product_attention>                             
                                                                                                                                                              torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_1                                              
                                                                                                                                                              torch/nn/modules/transformer.py(733): _mha_block                             
                                                                                                                                                                                                                                           
                    aten::mkldnn_convolution         3.81%     277.139ms         3.81%     277.196ms     277.196ms       1.68 Mb           0 b             1  <built-in method conv1d of type object at 0x7a2b604a4880>                    
                                                                                                                                                              torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                              torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                              nn.Module: Conv1d_7                                                          
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(396): forward              
                                                                                                                                                                                                                                           
                                aten::linear         0.09%       6.608ms         3.65%     265.555ms     462.639us     181.06 Mb           0 b           574  <built-in function linear>                                                   
                                                                                                                                                              torch/nn/functional.py(4727): _in_projection_packed                          
                                                                                                                                                              torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_1                                              
                                                                                                                                                                                                                                           
                                 aten::addmm         3.09%     224.382ms         3.47%     252.319ms     439.580us     181.06 Mb     181.06 Mb           574  <built-in function linear>                                                   
                                                                                                                                                              torch/nn/functional.py(4727): _in_projection_packed                          
                                                                                                                                                              torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_1                                              
                                                                                                                                                                                                                                           
--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------------------------------------------  
Self CPU time total: 7.266s