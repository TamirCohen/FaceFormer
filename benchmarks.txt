----------------------------- REGULAR RUN-----------------------


--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------------------------------------------  
                                        Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  Source Location                                                              
--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------------------------------------------  
                                aten::linear         0.19%      19.160ms        37.12%        3.659s      10.606ms       3.35 Gb           0 b           345  <built-in function linear>                                                   
                                                                                                                                                              torch/nn/modules/linear.py(113): forward                                     
                                                                                                                                                              nn.Module: Linear_77                                                         
                                                                                                                                                              faceformer.py(138): predict                                                  
                                                                                                                                                              demo.py(87): test_model                                                      
                                                                                                                                                                                                                                           
                                 aten::addmm        24.33%        2.398s        36.79%        3.626s      10.511ms       3.35 Gb       3.35 Gb           345  <built-in function linear>                                                   
                                                                                                                                                              torch/nn/modules/linear.py(113): forward                                     
                                                                                                                                                              nn.Module: Linear_77                                                         
                                                                                                                                                              faceformer.py(138): predict                                                  
                                                                                                                                                              demo.py(87): test_model                                                      
                                                                                                                                                                                                                                           
                                 aten::copy_        12.45%        1.227s        12.45%        1.227s       3.556ms           0 b           0 b           345  <built-in function linear>                                                   
                                                                                                                                                              torch/nn/modules/linear.py(113): forward                                     
                                                                                                                                                              nn.Module: Linear_77                                                         
                                                                                                                                                              faceformer.py(138): predict                                                  
                                                                                                                                                              demo.py(87): test_model                                                      
                                                                                                                                                                                                                                           
                                aten::conv1d         0.00%      16.000us         9.12%     899.157ms     899.157ms      35.99 Mb           0 b             1  <built-in method conv1d of type object at 0x7b41faca4880>                    
                                                                                                                                                              torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                              torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                              nn.Module: Conv1d_1                                                          
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(311): forward              
                                                                                                                                                                                                                                           
                           aten::convolution         0.00%      21.000us         9.12%     899.141ms     899.141ms      35.99 Mb           0 b             1  <built-in method conv1d of type object at 0x7b41faca4880>                    
                                                                                                                                                              torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                              torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                              nn.Module: Conv1d_1                                                          
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(311): forward              
                                                                                                                                                                                                                                           
                          aten::_convolution         0.00%      68.000us         9.12%     899.120ms     899.120ms      35.99 Mb           0 b             1  <built-in method conv1d of type object at 0x7b41faca4880>                    
                                                                                                                                                              torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                              torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                              nn.Module: Conv1d_1                                                          
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(311): forward              
                                                                                                                                                                                                                                           
                    aten::mkldnn_convolution         9.12%     898.933ms         9.12%     899.001ms     899.001ms      35.99 Mb           0 b             1  <built-in method conv1d of type object at 0x7b41faca4880>                    
                                                                                                                                                              torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                              torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                              nn.Module: Conv1d_1                                                          
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(311): forward              
                                                                                                                                                                                                                                           
          aten::_native_multi_head_attention         0.49%      47.844ms         7.23%     712.617ms       2.066ms      14.57 Mb    -574.22 Mb           345  ...in method _native_multi_head_attention of type object at 0x7b41faca4880>  
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_0                                              
                                                                                                                                                              torch/nn/modules/transformer.py(723): _sa_block                              
                                                                                                                                                              torch/nn/modules/transformer.py(681): forward                                
                                                                                                                                                                                                                                           
          aten::scaled_dot_product_attention         0.03%       3.326ms         5.07%     499.422ms       1.448ms      15.75 Mb    -312.64 Mb           345  <built-in function scaled_dot_product_attention>                             
                                                                                                                                                              torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_1                                              
                                                                                                                                                              torch/nn/modules/transformer.py(733): _mha_block                             


----------------------------- Optimized last layer -----------------------
--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------------------------------------------  
                                        Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  Source Location                                                              
--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------------------------------------------  
                                aten::linear         0.04%       4.388ms        14.20%        1.637s       5.704ms      76.76 Mb           0 b           287  <built-in function linear>                                                   
                                                                                                                                                              torch/nn/modules/linear.py(113): forward                                     
                                                                                                                                                              nn.Module: Linear_77                                                         
                                                                                                                                                              faceformer.py(138): predict                                                  
                                                                                                                                                              demo.py(87): test_model                                                      
                                                                                                                                                                                                                                           
                                 aten::addmm        13.98%        1.611s        14.14%        1.629s       5.677ms      76.76 Mb      76.76 Mb           287  <built-in function linear>                                                   
                                                                                                                                                              torch/nn/modules/linear.py(113): forward                                     
                                                                                                                                                              nn.Module: Linear_77                                                         
                                                                                                                                                              faceformer.py(138): predict                                                  
                                                                                                                                                              demo.py(87): test_model                                                      
                                                                                                                                                                                                                                           
                                aten::linear         0.04%       4.936ms        10.67%        1.230s       4.285ms     143.50 Kb           0 b           287  <built-in function linear>                                                   
                                                                                                                                                              torch/nn/modules/linear.py(113): forward                                     
                                                                                                                                                              nn.Module: Linear_78                                                         
                                                                                                                                                              faceformer.py(138): predict                                                  
                                                                                                                                                              demo.py(87): test_model                                                      
                                                                                                                                                                                                                                           
                                 aten::addmm        10.50%        1.210s        10.55%        1.216s       4.236ms     143.50 Kb     143.50 Kb           287  <built-in function linear>                                                   
                                                                                                                                                              torch/nn/modules/linear.py(113): forward                                     
                                                                                                                                                              nn.Module: Linear_78                                                         
                                                                                                                                                              faceformer.py(138): predict                                                  
                                                                                                                                                              demo.py(87): test_model                                                      
                                                                                                                                                                                                                                           
          aten::scaled_dot_product_attention         0.03%       3.118ms         7.76%     894.466ms       3.117ms      20.24 Mb    -361.91 Mb           287  <built-in function scaled_dot_product_attention>                             
                                                                                                                                                              torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_1                                              
                                                                                                                                                              torch/nn/modules/transformer.py(733): _mha_block                             
                                                                                                                                                                                                                                           
    aten::_scaled_dot_product_attention_math         0.14%      16.090ms         7.73%     891.348ms       3.106ms     382.15 Mb    -462.29 Mb           287  <built-in function scaled_dot_product_attention>                             
                                                                                                                                                              torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_1                                              
                                                                                                                                                              torch/nn/modules/transformer.py(733): _mha_block                             
                                                                                                                                                                                                                                           
          aten::_native_multi_head_attention         0.39%      44.959ms         7.00%     806.651ms       2.811ms      20.18 Mb    -413.99 Mb           287  ...in method _native_multi_head_attention of type object at 0x7d0ac30a4880>  
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_0                                              
                                                                                                                                                              torch/nn/modules/transformer.py(723): _sa_block                              
                                                                                                                                                              torch/nn/modules/transformer.py(681): forward                                
                                                                                                                                                                                                                                           
                                aten::conv1d         0.00%      12.000us         4.70%     542.172ms     542.172ms      35.99 Mb           0 b             1  <built-in method conv1d of type object at 0x7d0ac30a4880>                    
                                                                                                                                                              torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                              torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                              nn.Module: Conv1d_1                                                          
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(311): forward              
                                                                                                                                                                                                                                           
                           aten::convolution         0.00%      17.000us         4.70%     542.160ms     542.160ms      35.99 Mb           0 b             1  <built-in method conv1d of type object at 0x7d0ac30a4880>                    
                                                                                                                                                              torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                              torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                              nn.Module: Conv1d_1                                                          
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(311): forward              
                                                                                                                                                                                                                                           
                          aten::_convolution         0.00%      45.000us         4.70%     542.143ms     542.143ms      35.99 Mb           0 b             1  <built-in method conv1d of type object at 0x7d0ac30a4880>                    
                                                                                                                                                              torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                              torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                              nn.Module: Conv1d_1                                                          
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(311): forward              
                                                                                                                                                                                                                                           
                    aten::mkldnn_convolution         4.70%     541.999ms         4.70%     542.064ms     542.064ms      35.99 Mb           0 b             1  <built-in method conv1d of type object at 0x7d0ac30a4880>                    
                                                                                                                                                              torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                              torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                              nn.Module: Conv1d_1                                                          
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(311): forward              
                                                                                                                                                                                                                                           
                                aten::matmul         0.18%      21.211ms         3.96%     456.055ms     794.521us     381.87 Mb    -287.00 Kb           574  <built-in function scaled_dot_product_attention>                             
                                                                                                                                                              torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_1                                              
                                                                                                                                                              torch/nn/modules/transformer.py(733): _mha_block                             
                                                                                                                                                                                                                                           
                                aten::linear         0.10%      11.865ms         3.53%     407.399ms     709.754us     181.06 Mb           0 b           574  <built-in function linear>                                                   
                                                                                                                                                              torch/nn/functional.py(4727): _in_projection_packed                          
                                                                                                                                                              torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_1                                              
                                                                                                                                                                                                                                           
                                   aten::bmm         3.53%     406.556ms         3.53%     406.582ms     708.331us     382.15 Mb     382.15 Mb           574  <built-in function scaled_dot_product_attention>                             
                                                                                                                                                              torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_1                                              
                                                                                                                                                              torch/nn/modules/transformer.py(733): _mha_block                             
                                                                                                                                                                                                                                           
                                 aten::addmm         3.05%     351.096ms         3.32%     382.928ms     667.122us     181.06 Mb     181.06 Mb           574  <built-in function linear>                                                   
                                                                                                                                                              torch/nn/functional.py(4727): _in_projection_packed                          
                                                                                                                                                              torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_1                                              
                                                                                                                                                                                                                                           
                               aten::softmax         0.02%       2.601ms         2.62%     301.579ms       1.051ms     361.92 Mb     -26.91 Kb           287  <built-in function scaled_dot_product_attention>                             
                                                                                                                                                              torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_1                                              
                                                                                                                                                              torch/nn/modules/transformer.py(733): _mha_block                             
                                                                                                                                                                                                                                           
                              aten::_softmax         2.59%     299.012ms         2.59%     299.012ms       1.042ms     361.97 Mb     361.97 Mb           287  <built-in function scaled_dot_product_attention>                             
                                                                                                                                                              torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_1                                              
                                                                                                                                                              torch/nn/modules/transformer.py(733): _mha_block                             
                                                                                                                                                                                                                                           
                       aten::_masked_softmax         2.40%     276.846ms         2.44%     280.648ms     977.868us     120.86 Mb      -8.27 Kb           287  ...in method _native_multi_head_attention of type object at 0x7d0ac30a4880>  
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_0                                              
                                                                                                                                                              torch/nn/modules/transformer.py(723): _sa_block                              
                                                                                                                                                              torch/nn/modules/transformer.py(681): forward                                
                                                                                                                                                                                                                                           
                                aten::conv1d         0.00%      11.000us         2.31%     266.099ms     266.099ms      17.99 Mb           0 b             1  <built-in method conv1d of type object at 0x7d0ac30a4880>                    
                                                                                                                                                              torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                              torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                              nn.Module: Conv1d_2                                                          
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(311): forward              


----------------------------- Optimized last layer + dynamic Quantization-----------------------


--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------------------------------------------  
                                        Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  Source Location                                                              
--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------------------------------------------  
          aten::_native_multi_head_attention         0.98%      76.277ms        14.26%        1.106s       3.206ms      14.57 Mb    -574.86 Mb           345  ...in method _native_multi_head_attention of type object at 0x7b86068a4880>  
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_0                                              
                                                                                                                                                              torch/nn/modules/transformer.py(723): _sa_block                              
                                                                                                                                                              torch/nn/modules/transformer.py(681): forward                                
                                                                                                                                                                                                                                           
                                aten::conv1d         0.00%      17.000us        14.23%        1.104s        1.104s      35.99 Mb           0 b             1  <built-in method conv1d of type object at 0x7b86068a4880>                    
                                                                                                                                                              torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                              torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                              nn.Module: Conv1d_1                                                          
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(311): forward              
                                                                                                                                                                                                                                           
                           aten::convolution         0.00%      20.000us        14.23%        1.104s        1.104s      35.99 Mb           0 b             1  <built-in method conv1d of type object at 0x7b86068a4880>                    
                                                                                                                                                              torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                              torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                              nn.Module: Conv1d_1                                                          
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(311): forward              
                                                                                                                                                                                                                                           
                          aten::_convolution         0.00%      49.000us        14.23%        1.104s        1.104s      35.99 Mb           0 b             1  <built-in method conv1d of type object at 0x7b86068a4880>                    
                                                                                                                                                              torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                              torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                              nn.Module: Conv1d_1                                                          
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(311): forward              
                                                                                                                                                                                                                                           
                    aten::mkldnn_convolution        14.23%        1.104s        14.23%        1.104s        1.104s      35.99 Mb           0 b             1  <built-in method conv1d of type object at 0x7b86068a4880>                    
                                                                                                                                                              torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                              torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                              nn.Module: Conv1d_1                                                          
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(311): forward              
                                                                                                                                                                                                                                           
          aten::scaled_dot_product_attention         0.05%       4.112ms         9.98%     774.068ms       2.244ms      14.59 Mb    -314.18 Mb           345  <built-in function scaled_dot_product_attention>                             
                                                                                                                                                              torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_1                                              
                                                                                                                                                              torch/nn/modules/transformer.py(733): _mha_block                             
                                                                                                                                                                                                                                           
    aten::_scaled_dot_product_attention_math         0.32%      25.082ms         9.93%     769.956ms       2.232ms     328.77 Mb    -357.83 Mb           345  <built-in function scaled_dot_product_attention>                             
                                                                                                                                                              torch/nn/functional.py(5017): multi_head_attention_forward                   
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_1                                              
                                                                                                                                                              torch/nn/modules/transformer.py(733): _mha_block                             
                                                                                                                                                                                                                                           
                                aten::select         8.13%     630.540ms         9.34%     724.070ms       6.066us           0 b           0 b        119370  faceformer.py(35): enc_dec_mask                                              
                                                                                                                                                              faceformer.py(138): predict                                                  
                                                                                                                                                              demo.py(87): test_model                                                      
                                                                                                                                                              torch/utils/_contextlib.py(115): decorate_context                            
                                                                                                                                                              demo.py(270): main                                                           
                                                                                                                                                                                                                                           
                       aten::_masked_softmax         6.61%     512.752ms         6.69%     518.787ms       1.504ms     209.77 Mb           0 b           345  ...in method _native_multi_head_attention of type object at 0x7b86068a4880>  
                                                                                                                                                              torch/nn/modules/activation.py(1021): forward                                
                                                                                                                                                              nn.Module: MultiheadAttention_0                                              
                                                                                                                                                              torch/nn/modules/transformer.py(723): _sa_block                              
                                                                                                                                                              torch/nn/modules/transformer.py(681): forward                                
                                                                                                                                                                                                                                           
                                aten::conv1d         0.00%      20.000us         6.53%     506.792ms     506.792ms      17.99 Mb           0 b             1  <built-in method conv1d of type object at 0x7b86068a4880>                    
                                                                                                                                                              torch/nn/modules/conv.py(304): _conv_forward                                 
                                                                                                                                                              torch/nn/modules/conv.py(312): forward                                       
                                                                                                                                                              nn.Module: Conv1d_2                                                          
                                                                                                                                                              transformers/models/wav2vec2/modeling_wav2vec2.py(311): forward              